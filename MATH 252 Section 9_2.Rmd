<style type="text/css">
    ol { list-style-type: lower-alpha; }
</style>

# CHAPTER 9  

## Section 9.2  Relative Efficiency  

### "Essentially, a more efficient estimator, experiment, or test needs fewer observations than a less efficient one to achieve a given performance."  

### Main Topics   

- Definition 9.1 Given two unbiased estimators, $\hat{\theta_1}$ and $\hat{\theta_2}$, of a parameter $\theta$, with variances $V(\hat{\theta_1})$ and $V(\hat{\theta_2})$, respectively, then the efficiency of $\hat{\theta_1}$ relative to $\hat{\theta_2}$, denoted  $eff(\hat{\theta_1}. \hat{\theta_2})$, is defined to be the ratio  

$\quad\quad\quad\quad eff(\hat{\theta_1}, \hat{\theta_2}) = \frac{V(\hat{\theta_2})}{V(\hat{\theta_1})}$  

- Ineresting fact: the variance of the sample median is $(1.2533)^2(\sigma^2/n)$  

### Exercises  
  
#### 9.1 In Exercise 8.4 we considered a random sample of size three from an exponential distribution with density function given by  
  
#### $\quad\quad\quad\quad f(y) = \begin{cases} \left(\frac{1}{\theta}\right)e^{-y/\theta} & 0 < y \\ 0 & \text{elsewhere} \end{cases} \quad\quad$ 
  
#### and determined that $\hat{\theta_1}=Y_1, \hat{\theta_2} = (Y_1+Y_2)/2, \hat{\theta_3} = (Y_1 + 2Y_2)/3, \text{ and } \hat{\theta_5} = \overline{Y}$ are all unbiased estimators for $\theta$. Find the efficiency of $\hat\theta_1$ relative to $\hat{\theta_5}$, of $\hat\theta_2$ relative to $\hat{\theta_5}$, and of $\hat\theta_3$ relative to $\hat{\theta_5}$    

We are given a random sample $Y_1,Y_2,Y_3$ from an exponential distribution, so  $E(Y_i) = \theta$ and $V(Y_i) = \theta^2$  

$V(\hat{\theta_1}) = V(Y_1) = \theta^2$  

$V(\hat{\theta_2}) = V((Y_1+Y_2)/2)= 1/4(V(Y_1) + V(Y_2)) = \theta^2/2$  
  
$V(\hat{\theta_3}) = V((Y_1+2Y_2)/3) = 1/9(V(Y_1) + 4V(Y_2)) = 5\theta^2/9$  

$V(\hat{\theta_5}) = V(\overline{Y}) = \theta^2/3$  

Using these  

$eff(\hat{\theta_1}, \hat{\theta_5}) = 1/3$  
  
$eff(\hat{\theta_2}, \hat{\theta_5}) = 2/32$  
  
$eff(\hat{\theta_3}, \hat{\theta_5}) = 3/5$  

--  
  
#### 9.2  Let $Y_1,...,Y_n$ denote a random sample from a population with mean $\mu$ and variance $\sigma^2$. Consider the following three estimates for $\mu$.  

#### $\quad\quad \hat{\mu_1} = \frac{1}{2}(Y_1+Y_2) \quad\quad  \hat{\mu_2} = \frac{1}{4}Y_1 + \frac{Y_2,...,Y_{n-1}}{2(n-2)} + \frac{1}{4}Y_n \quad\quad \hat{\mu_3}= \overline{Y}$  

a) Show that each of the three estimators is unbiased.  

    
    $E(\hat{\mu_1}) = E(\frac{1}{2}(Y_1+Y_2)) = \frac{1}{2}(E(Y_1) + E(Y_2)) = \mu$  
    
    $E(\hat{\mu_2}) = E(\frac{1}{4}Y_1 + \frac{Y_2,...,Y_{n-1}}{2(n-2)} + \frac{1}{4}Y_n) = \frac{1}{4}E(Y_1) + \frac{\sum\limits_{i = 2}^{n-2}E(Y_i)}{2(n-2)} + \frac{1}{4}E(Y_n) = \frac{1}{2}\mu + \frac{(n-2)\mu}{2(n-2)} = \mu$  
    
    $E(\hat{\mu_3})= E(\overline{Y}) = \mu$  
    
b) Find the efficiency of $\hat{\mu_3}$ relative to $\hat{\mu_2}$ and $\hat{\mu_1}$.  

    $V(\hat{\mu_1}) = V(\frac{1}{2}(Y_1+Y_2)) = \frac{1}{4}(V(Y_1) + V(Y_2)) = \frac{\sigma^2}{2}$  
    
    $V(\hat{\mu_2}) = V(\frac{1}{4}Y_1 + \frac{Y_2,...,Y_{n-1}}{2(n-2)} + \frac{1}{4}Y_n) = \frac{1}{16}V(Y_1) + \frac{\sum\limits_{i = 2}^{n-2}V(Y_i)}{4(n-2)^2} + \frac{1}{16}V(Y_n) = \frac{1}{8}\sigma^2 + \frac{(n-2)\sigma^2}{4(n-2)^2} = \frac{1}{8}\sigma^2 + \frac{1}{4(n-2)}\sigma^2$  
    
    $V(\hat{\mu_3})= V(\overline{Y}) = \frac{\sigma^2}{n}$  
    
    $eff(\hat{\mu_3}, \hat{\mu_1}) = \frac{V(\hat{\mu_1})}{V(\hat{\mu_3})} = \frac{\frac{\sigma^2}{2}}{\frac{\sigma^2}{n}} = \frac{n}{2}$  
    
    $eff(\hat{\mu_3}, \hat{\mu_2}) = \frac{V(\hat{\mu_2})}{V(\hat{\mu_3})} = \frac{\frac{1}{8}\sigma^2 + \frac{1}{4(n-2)}\sigma^2}{\frac{\sigma^2}{n}} = \frac{n}{8} + \frac{n}{4(n-2)} = \frac{n^2}{8(n-2)} $  
    
--  
  
#### 9.3 Let $Y_1,...,Y_n$ denote a random sample from the uniform distribution on the interval $(\theta, \theta + 1)$. Let   

#### $\quad\quad\quad\quad \hat{\theta_1} = \overline{Y} - \frac{1}{2} \quad \quad \hat{\theta_2} = Y_{(n)} - \frac{n}{n+1}$    
  
a) Show that both $\hat{\theta_1}$ and $\hat{\theta_2}$ are unbiased estimators of $\theta$  

    
    $E(\hat{\theta_1}) = E(\overline{Y} - \frac{1}{2}) = E(\overline{Y}) - E(\frac{1}{2}) = E(\overline{Y}) - \frac{1}{2} = \mu - \frac{1}{2}$  
    
    I will calculate the mean of this uniform distribution for pratice.  
    
    The density function of a uniform distribution is given by $f(y) = \frac{1}{\theta_2 - \theta_1}, \text{ for } \theta_1 \leq y \leq \theta_2$.  So in this case $f(y) = \frac{1}{\theta + 1 - \theta} = 1$  so  
    
    $E(Y_i) = \int\limits_{\theta}^{\theta + 1}ydy = \frac{1}{2}y^2 \bigg{|}_{\theta}^{\theta + 1} = \frac{1}{2}((\theta + 1)^2 - \theta^2) = \theta + \frac{1}{2}$  
    
    therefore  
    
    $E(\hat{\theta_1}) = \mu - \frac{1}{2} = \theta$  So $\hat{\theta_1}$ is unbiased.   
      
    $E(\hat{\theta_2}) = E(Y_{(n)} - \frac{n}{n+1}) = E(Y_{(n)}) - \frac{n}{n+1}$
    
    need to compute $E(Y_{(n)})$  I will do this for practice.  
    
    The density function for $Y_{(n)}$ is given by $g_{(n)}(y) = n[F(y)]^{n-1}f(y)$.  From above 
    
    $f(y) = 1$ so $F(y) = \int\limits_{\theta}^{y}1dx = y - \theta$  so $g_{(n)}(y) = n(y - \theta)^{n-1}$
    
    
    $E(Y_{(n)}) = \int\limits_{\theta}^{\theta + 1}ny(y - \theta)^{n-1}dy \quad$  
      
    integration by parts  
    
    $u = ny \;\Rightarrow \; du = n \quad \quad dv = (y-\theta)^{n-1} \;\Rightarrow\; v = \frac{1}{n}(y -\theta)^n$  
    
    $= ny \frac{1}{n}(y-\theta)^n \big|_{\theta}^{\theta + 1}- \int\limits_{\theta}^{\theta + 1}(y - \theta)^ndy = \theta + 1 - \frac{1}{n+1} = \theta + \frac{n}{n+1}$    
    
    So  
    
    $E(\hat{\theta_2}) = E(Y_{(n)} - \frac{n}{n+1}) = E(Y_{(n)}) - \frac{n}{n+1} = \theta + \frac{n}{n+1} - \frac{n}{n+1} = \theta$  
      
  
b) Find the efficiency of $\hat{\theta_1}$ with respect to $\hat{\theta_2}$  

    
    $V(\hat{\theta_1}) = \sigma^2/n$.  
      
    So need to calculate $\sigma^2$ for the uniform distribution.  I will do this for practice  
    
    $V(Y_i) = E(Y^2) - E(Y)^2 = E(Y^2) - \theta^2/4$  
    
    $E(Y_i^2) = \int\limits_{\theta}{\theta + 1}y^2dy = \frac{1}{3}y^3\big|_{\theta}^{\theta + 1} = \frac{1}{3}((\theta + 1)^3 - \theta^3)) = \frac{1}{3}((\theta + 1 - \theta)((\theta + 1)^2 + (\theta + 1)\theta + \theta^2)$   
    $= \frac{1}{3}(\theta^2 + \theta + 1 + 2\theta^2 + 2\theta + \theta^2) = \frac{1}{3}(3\theta^2 + 3\theta + 1)$    
  
         
    $V(Y_i) = E(Y^2) - E(Y)^2 = E(Y^2) - (\theta^2 + \theta + \frac{1}{4}) =  \frac{1}{3}(3\theta^2 + 3\theta + 1) - (\theta^2 + \theta + \frac{1}{4})$   
    $=  \frac{1}{3}(3\theta^2 + 3\theta + 1) - \frac{1}{4}(4\theta^2 + 4\theta + 1) = \frac{1}{12}$.
    
    So  
    $V(\hat{\theta_1}) = 1/12n$.  
    
    $V(\hat{\theta_2}) = V(Y_{(n)} - \frac{n}{n+1}) = V(Y_{(n)}) = E(Y_{(n)}^2) - E(Y_{(n)})^2$  
    
    
    $E(Y_{(n)}^2) = \int\limits_{\theta}^{\theta + 1}ny^2(y - \theta)^{n-1}dy \quad$ 
    
    integration by parts  
    
    $u = ny^2 \;\Rightarrow \; du = 2ny \quad \quad dv = (y-\theta)^{n-1} \;\Rightarrow\; v = \frac{1}{n}(y -\theta)^n$ 

    $= ny^2 \frac{1}{n}(y-\theta)^n \big|_{\theta}^{\theta + 1}- \int\limits_{\theta}^{\theta + 1}2y(y - \theta)^ndy = (\theta + 1)^2 - \int\limits_{\theta}^{\theta + 1}2y(y - \theta)^ndy$  
    
    integration by parts  
    
    $u = 2y \;\Rightarrow \; du = 2 \quad \quad dv = (y-\theta)^{n} \;\Rightarrow\; v = \frac{1}{n+1}(y-\theta)^{n+1}$  
    
    $(\theta + 1)^2 - \frac{2(\theta + 1)}{n+1} + \frac{2}{n+1}\int\limits_{\theta}^{\theta + 1}(y - \theta)^{n+1}dy = (\theta + 1)^2 - \frac{2(\theta + 1)}{n+1} + \frac{2}{(n+1)(n+2)}$ 
    
    $V(\hat{\theta_2}) = (\theta + 1)^2 - \frac{2(\theta + 1)}{n+1} + \frac{2}{(n+1)(n+2)} - (\theta + \frac{n}{n+1})^2$  
    
    Note: 
    
    $E(y_{(n)})^2 = \left(\theta + \frac{n}{n+1}\right)^2 = \left((\theta + 1) + (\frac{n}{n+1} - 1)\right)^2 = \left((\theta + 1) - \left(\frac{1}{n+1}\right)\right)^2 = (\theta + 1)^2 - \frac{2(\theta + 1)}{n+1} + \frac{1}{(n+1)^2}$  
    
    $V(\hat{\theta_2}) = (\theta + 1)^2 - \frac{2(\theta + 1)}{n+1} + \frac{2}{(n+1)(n+2)} - (\theta + 1)^2 + \frac{2(\theta + 1)}{n+1} - \frac{1}{(n+1)^2} = \frac{2}{(n+1)(n+2)} - \frac{1}{(n+1)^2} = \frac{n}{(n+1)^2(n+2)}$    
    
    
    $eff(\hat{\theta_1}, \hat{\theta_2}) = \frac{V(\hat{\theta_2})}{V(\hat{\theta_1})} = \frac{\frac{n}{(n+1)^2(n+2)}}{\frac{1}{12n}} = \frac{12n^2}{(n+1)^2(n+2)}$  
    
--  
  
#### 9.4  Let $Y_1,...,Y_n$ denote a random sample of size n from a uniform distribution on the interval $(0,\theta)$.  If $Y_{(1)} = min(Y_1,...,Y_n)$, the result of Exercise 8.18 is that $\hat{\theta_1} = (n+1)Y_{(1)}$ is an unbiased estimator for $\theta$. If $Y_{(n)} = max(Y_1,...,Y_n)$, the results of Example 9.1 imply that $\hat{\theta_2} = \frac{n+1}{n}Y_{(n)}$ is another unbiased estimator for $\theta$. Show that the efficiency of $\hat{\theta_1}$  relative to $\hat{\theta_2}$ is $\frac{1}{n^2}$. Notice that this implies that $\hat{\theta_2}$ is a markedly superior estimator.
    
The density function for $Y_{(1)}$ is given by $g_{Y_{(1)}}(y) = n[1 - F(Y)]^{n-1}f(y)$  

Will do calculations for practice.  

$f(y) = \frac{1}{\theta}$ , so $F(y) = \int\limits_{0}^{y}\frac{1}{\theta}dx = \frac{y}{\theta}$  

$g_{Y_{(1)}}(y) = n[1 - F(Y)]^{n-1}f(y) = n(1 - \frac{y}{\theta})^{n-1}\frac{1}{\theta} = \frac{n}{\theta^2}(\theta - y)^{n-1}$   

$E(Y_{(1)}) = \frac{n}{\theta^n}\int\limits_0^{\theta}y(\theta-y)^{n-1}dy$  

$u = y \;\Rightarrow \; du = 1 \quad \quad dv = (\theta - y)^{n-1} \;\Rightarrow\; v = -\frac{1}{n}(\theta - y)^n$ 

$E(Y_{(1)}) = \frac{n}{\theta^n}\left[ -\frac{y}{n}(\theta - y)^n \big{|}_{0}^{\theta} + \frac{1}{n}\int\limits_0^{\theta}(\theta - y)^ndy \right] = -\frac{n}{\theta^n}\frac{1}{n(n+1)}(\theta - y)^{n+1} \big{|}_{0}^{\theta} = \frac{\theta}{n+1}$  

$E(Y_{(1)}^2) = \frac{n}{\theta^n}\int\limits_0^{\theta}y^2(\theta-y)^{n-1}dy$   

$u = y^2 \;\Rightarrow \; du = 2y \quad \quad dv = (\theta - y)^{n-1} \;\Rightarrow\; v = -\frac{1}{n}(\theta - y)^n$  

$E(Y_{(1)}^2) = -\frac{y^2}{\theta^n}(\theta - y)^n\big|_0^{\theta} + \frac{2}{\theta^n}\int\limits_0^{\theta}y(\theta-y)^ndy$   

$u = y \;\Rightarrow \; du = 1  \quad \quad dv = (\theta - y)^{n} \;\Rightarrow\; v = -\frac{1}{n+1}(\theta - y)^{n+1}$  

$E(Y_{(1)}^2) = -\frac{2}{\theta^n}\left[\frac{y}{n+1}(\theta - y)^{n+1}\big|_0^{\theta} + \int\limits_0^{\theta}\frac{1}{n+1}(\theta - y)^{n+1}\right] = -\frac{2}{\theta^n}\frac{1}{(n+1)(n+2)}(\theta - y)^{n+2}\big|_0^{\theta} = \frac{2\theta^2}{(n+1)(n+2)}$  

$V(\hat{\theta_1}) = (n+1)^2V(Y_{(1)}) = (n+1)^2\left[E(Y_{(1)}^2) - E(Y_{(1)})^2 \right] = (n+1)^2\left[\frac{2\theta^2}{(n+1)(n+2)} -  \frac{\theta^2}{(n+1)^2}\right] = \frac{n\theta^2}{n+2}$   


$E(Y_{(n)}) = \int\limits_{0}^{\theta}\frac{n}{\theta^n}y^{n}dy = \frac{n}{(n+1)\theta^n}y^{n+1}\big|_0^{\theta} = \frac{n\theta}{n+1}$

$E(Y_{(n)}^2) = \int\limits_{0}^{\theta}\frac{n}{\theta^n}y^{n+1}dy = \frac{n}{(n+2)\theta^n}y^{n+2}\big|_0^{\theta} = \frac{n\theta^2}{n+2}$  

$V(\hat{\theta_2}) = \frac{(n+1)^2}{n^2}\left(\frac{n\theta^2}{n+2} - \frac{n^2\theta^2}{(n+1)^2} \right) = \frac{(n+1)^2}{n^2} \left(\frac{n\theta^2((n+1)^2 -(n+2)n)}{(n+1)^2(n+2)}\right) =\frac{(n+1)^2}{n^2}\frac{n\theta^2}{(n+1)^2(n+2)} = \frac{\theta^2}{n(n+2)}$

$eff(\hat{\theta_1}, \hat{\theta_2}) = \frac{V(\hat{\theta_2})}{V(\hat{\theta_1})} = \frac{\frac{\theta^2}{n(n+2)}}{\frac{n\theta^2}{n+2}} = \frac{1}{n^2}$  

--  
  
#### 9.5  Suppose    $Y_1,...,Y_n$ is a random sample from a normal distribution with mean $\mu$ and variance $\sigma^2$. Two unbiased estimators of $\sigma^2$ are   

#### $\hat{\sigma_1}^2 = S^2 = \frac{1}{n-1}\sum\limits_{i = 1}^n(Y_i - \overline{Y})^2 \quad\quad \hat{\sigma_2}^2 = \frac{1}{2}(Y_1 - Y_2)^2$  

#### Find the efficiency of $\hat{\sigma_1}^2$ relative to $\hat{\sigma_2}^2$  

Since $\frac{n-1}{\sigma^2}\hat{\sigma_1}^2$ is $\chi_{n-1}^2$, we have $V(\frac{n-1}{\sigma^2}\hat{\sigma_1}^2) = 2(n-1)$ and $V(\frac{n-1}{\sigma^2}\hat{\sigma_1}^2) = \frac{(n-1)^2}{\sigma^4}V(\hat{\sigma_1}^2)$ so $V(\hat{\sigma_1}^2) = \frac{2\sigma^4}{n-1}$   


$V(\hat{\sigma_2}^2) = V(\frac{1}{2}(Y_1 - Y_2)^2) = \frac{1}{4}V((Y_1 - Y_2)^2)$  

Not $U = Y_1-Y_2$ is a normal distribution with mean $\mu_U = \mu - \mu = 0$ and variance %\sigma_u = sqrt{2}\sigma$, since $Y_1$ and $Y_2$ are independent.  So  $Z = ${Y_1 - Y_2}{sqrt{2}\sigma} and $Z^2 = \frac{(Y_1 - Y_2)^2}{2 \sigma^2}$ is $\chi^2$ with 1 degree of freedom.  So  

$\frac{1}{\sigma_4}V(\frac{(Y_1 - Y_2)^2}{2}) = V(\frac{(Y_1 - Y_2)^2}{2\sigma^2}) = 2$ therefore $V(\frac{(Y_1 - Y_2)^2}{2}) = 2\sigma^4$    

$eff(\hat{\sigma_1^2}, \hat{\sigma_2^2}) = \frac{2\sigma^4}{\frac{2\sigma^4}{n-1}} = n-1$  

--  
  
#### 9.6  Suppose that $Y_1,...,Y_n$ denotes a random sample of size n from a Poisson distribution with mean $\lambda$. Consider $\hat{\lambda_1} = (Y_1 + Y_2)/2$ and  $\hat{\lambda_2} = \overline{Y}$.  Derive the efficiency of $\hat{\lambda_1}$ relative to $\hat{\lambda_2}$.  

We know  

$V(\hat{\lambda_1}) = \frac{1}{4}(V(Y_1) + V(Y_2)) = \frac{\lambda}{2}$   

$V(\overline{Y}) = \frac{\lambda}{n}$  

So  

$eff(\hat{\lambda_1}, \hat{\lambda_2}) = \frac{\frac{\lambda}{n}}{\frac{\lambda}{2}} = \frac{2}{n}$  

--  
  
#### 9.7  Suppose that $Y_1,...,Y_n$ denotes a random sample of size n from an exponential distribution with density function given by  

#### $f(y) = \begin{cases} \frac{1}{\theta}e^{-\frac{y}{\theta}} & 0 < y \\ 0 & \text{ elsewhere } \end{cases}$   

#### In Exercise 8.15, we determined that $\hat{\theta_1} = nY{(1)}$ is an unbiased estimator of $\theta$ with $MSE(\hat{\theta_1}) = \theta^2$.  Consider the estimator $\hat{theta_2} = \overline{Y}$, and find the efficiency of $\hat{\theta_1}$ relative to $\hat{\theta_2}$.  

Note: $MSE(\hat{\theta_1}) = V(\hat{\theta_1}) + (B(\hat{\theta_1}))^2$.  Since $\hat{\theta_1}$ is unbiased, $B(\hat{\theta_1}) = 0$, so $\theta^2 =  MSE(\hat{\theta_1}) = V(\hat{\theta_1})$  

since $Y_1,...,Y_n$ are i.i.d.  
$V(\overline{Y}) = \frac{V(Y_1)}{n} = \frac{\theta^2}{n}$  

$eff(\hat{\theta_1}, \hat{\theta_2}) = \frac{\frac{\theta^2}{n}}{\theta^2} = \frac{1}{n}$  

--  
  
#### 9.8  Let $Y_1,...,Y_n$ denote a random sample from a probability density function $f(y)$, which has unknown parameter $\theta$. If $\hat{\theta}$ is an unbiased estimator of $\theta$, then under very general conditions   
  
#### $V(\hat{\theta}) \geq I(\theta)$, where $\quad I(\theta) = \left[nE\left( - \frac{\partial^2 ln(f(y))}{\partial\theta^2}\right) \right]^{-1}$  
  
#### (This is known as the Cramer-Rao inequality.)  If $V(\hat{\theta}) = I(\theta)$, the estimate $\hat{\theta}$ is said to be efficient.    


a) Suppose the $f(y)$ is the normal density with mean $\mu$ and variance $\sigma^2$.  Show that $\overline{Y}$ is an efficient estimator of $\mu$.  

    $V(\overline{Y}) = \frac{\sigma^2}{n}$ where n is the sample size.  So we need to show that $I(\theta) = \frac{\sigma^2}{n}$  

    $ln(f(y)) = ln(\frac{1}{\sqrt{2\pi} \sigma}e^{-\frac{(y - \mu)^2}{2\sigma^2}}) = ln \left(\frac{1}{\sqrt{2\pi} \sigma} \right) - \frac{(y - \mu)^2}{2\sigma^2}$

    $\frac{\partial^2}{\partial\mu^2}\left(n \left(\frac{1}{\sqrt{2\pi} \sigma} \right) - \frac{(y - \mu)^2}{2\sigma^2}\right) = \frac{\partial}{\partial\mu}\left(\frac{1}{\sigma^2}(y - \mu)\right) = -\frac{1}{\sigma^2}$  

    It follows

    $I(\theta) = \left[nE\left(\frac{1}{\sigma^2} \right) \right]^{-1} = \frac{\sigma^2}{n}$ as was to be shown  
  
b) This inequality also holds for discrete probability functions $p(y)$.  Suppose that $p(y)$ is the Poisson probabiity function with mean $\lambda$.  Show that $\overline{Y}$ is an efficient estimator of $\lambda$.  

    Let $Y_1,...,Y_n$ denote a random sample from a Poisson probability density function.  
    
    $p(y) = \frac{\lambda^ye^{-\lambda}}{y!}$, for $y = 0,1,2,3...$.   
    
    $V(\overline{Y}) = \frac{\lambda}{n}$, so we need to show that $I(\lambda) = \frac{\lambda}{n}$
    
    $ln(\frac{\lambda^ye^{-\lambda}}{y!}) = ln(\lambda^y) + ln(e^{-\lambda}) - ln(y!) = ln(\lambda^y) + -\lambda - ln(y!)$   
    
    $\frac{\partial^2}{\partial\mu^2}(ln(\lambda^y) + -\lambda - ln(y!)) = \frac{\partial}{\partial\lambda}(\frac{y\lambda^{y-1}}{\lambda^y} - 1) = -\frac{y}{\lambda^2}$  
    
    $I(\lambda) = \left(nE\left[\frac{Y}{\lambda^2}\right]\right)^{-1} = \frac{\lambda}{n}$, since Y is Poisson and so $E[Y] = \lambda$  
    
--  
  
  

    
    


