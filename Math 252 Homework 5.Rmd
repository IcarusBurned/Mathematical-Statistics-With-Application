---
title: "Math 252 Homework 5"
author: "Matthew Schroeder"
date: "October 9, 2018"
output:
  html_document: default
  pdf_document: default
---

<style type="text/css">
    ol { list-style-type: upper-alpha; }
</style>


### Chapter 7

### 7.16  

Suppose that $W_1$ and $W_2$ are are independent $\chi^2$-distributed random variables with $\nu_1$ and $\nu_2$  degrees of freedom, respectively. According to Definition 7.3,   

$\quad \quad \quad \quad F = \frac{W_1/\nu_1}{W_2/\nu_2}$   

has an $F$ distribution $\nu_1$ and $\nu_2$ numerartor and denomenator degrees of freedom, respectively.  Use the preceeding structure of $F$, the independence of $W_1$ and $W_2$, and the result summarized in Exercise 7.14(b) to show  

The result summarized in Exercise 7.14(b) is:  Given Y a $\chi^2$-distribution with $\nu$ degrees of freedom, then   

$\quad (*) \quad E(Y^a) = \frac{\Gamma([\nu/2] + a)}{\Gamma([\nu/2])}2^a \;$ if $\nu > -2a$

a: $E(F) = \frac{\nu_2}{\nu_2 - 2} \quad$ if $\nu_2 > 2$  

$\textbf{Solution:}$  

Since $W_1$ and $W_2$ are independent, so are $W_1$ and $\frac{1}{W_2}$, so  

$E(F) = \frac{\nu_2}{\nu_1}E(\frac{W_1}{W_2}) = \frac{\nu_2}{\nu_1}E(W_1)E(W_2^{-1}) = \frac{\nu_2}{\nu_1}\nu_1E(W_2^{-1}) = \nu_2E(W_2^{-1})$  

If $\nu_2 > 2$ then $(*)$ gives   

$E(F) = \nu_2 \frac{\Gamma([\nu_2/2] -1)}{\Gamma([\nu_2/2])}2^{-1}$  

using the identity $\Gamma(x) = (x-1)\Gamma(x-1)$ we have $\Gamma([\nu_2/2] -1) = \frac{\Gamma(\nu_2/2)}{\nu_2/2 -1}$  so

$\frac{\Gamma([\nu_2/2] -1)}{\Gamma([\nu_2/2])} = \frac{2}{\nu_2 - 2}$  and finally     

$E(F) = \nu_2 \frac{\Gamma([\nu_2/2] -1)}{\Gamma([\nu_2/2])}2^{-1} = \nu_2\frac{2}{\nu_2 - 2}\frac{1}{2} = \frac{\nu_2}{\nu_2 - 2} \;$ where, as mentioned above, $\nu_2 > 2$  

--

b: $V(F) = \left[ 2\nu_2^2(\nu_1 + \nu_2 - 2)\right] / \left[ \nu_1(\nu_2 - 2)^2(\nu_2 - 4)\right] \quad$, if $\nu_2 > 4$    

$\textbf{Solution:}$ 


Since $W_1$ and $W_2$ are independent, so are $W_1^2$ and $\frac{1}{W_2^2}$, so using $(*)$ and the result of part (a) gives that   

For $\nu_2 > 4$  

$V(F) = \left( \frac{\nu_2}{\nu_1} \right)^2 \left(E(W_1^2W_2^{-2}) - E(W_1W_2^{-1})^2 \right) = \left(\frac{\nu_2}{\nu_1}\right)^2 \left(E(W_1^2)E(W_2^{-2}) -\left(\frac{\nu_2}{\nu_2 - 2}\right)^2 \right)$  

$\quad\quad\quad = \left(\frac{\nu_2}{\nu_1}\right)^2 \left(4\frac{\Gamma([\nu_1/2] +2)}{\Gamma([\nu_1/2])}\frac{1}{4}\frac{\Gamma([\nu_2/2] -2)}{\Gamma([\nu_2/2])}\right) - \left(\frac{\nu_2}{\nu_2 - 2}\right)^2= \left(\frac{\nu_2}{\nu_1}\right)^2 \left( \frac{(\nu_1/2+1)(\nu_1/2)}{(\nu_2/2 -1)(\nu_2/2 -2)}\right) - \left(\frac{\nu_2}{\nu_2 - 2}\right)^2$  

$\quad\quad\quad = \frac{\nu_2^2}{\nu_1}\frac{(\nu_1+2)}{(\nu_2 -2)(\nu_2 -4 )} - \frac{\nu_2^2}{(\nu_2 - 2)^2} = \frac{\nu_2^2((\nu_2 - 2)(\nu_1 + 2) - \nu_1(\nu_2 -4))}{\nu_1(\nu_2 - 2)^2(\nu_2 - 4)} = \frac{\nu_2^2(\nu_1\nu_2 - 2\nu_1 +2\nu_2 -4 - \nu_1\nu_2 + 4\nu_1)}{\nu_1(\nu_2 - 2)^2(\nu_2 - 4)} = \frac{2\nu_2^2(\nu_1 + \nu_2 - 2)}{\nu_1(\nu_2 - 2)^2(\nu_2 - 4)}$  
  
Writing nasty algebra in latex is not fun.  


--

### 7.25  

Workers employed in a large service industry have an average wage of $7.00 per hour with
a standard deviation of $.50. The industry has 64 workers of a certain ethnic group. These
workers have an average wage of $6.90 per hour. Is it reasonable to assume that the wage rate
of the ethnic group is equivalent to that of a random sample of workers from those employed
in the service industry? [Hint: Calculate the probability of obtaining a sample mean less than
or equal to $6.90 per hour.]

$\textbf{Solution:}$   

We have a population with a $\mu = 7.00$ and $\sigma = 0.50$. Let $\overline{Y}$ denote the mean of 100 samples from this population.  By the central limit theorem (assuming $n=64$ is large enough to apply the CLT),  

$P(\overline{Y} \leq 6.9) \approx P(Z \leq \frac{\sqrt{64}(6.90 - 7.00)}{0.5} = P(Z \leq -1.6)$

Using the table in the back of the book gives $P(\overline{Y} \leq 6.9) \approx 0.0548$ 

If we take the cutoff for statistical significance to be 1 in 20 or 0.05, then since 0.0548 is above this cutoff, we would say it is reasonable to assume that the wage rate of the ethnic group is equivalent to that of a random sample of workers.  

--

### 7.38   

Suppose that $X_1, X_2,...,X_n$ and $Y_1, Y_2,...,Y_n$ are independent random samples from populations with means $\mu_1$ and $\mu_2$ and variances $\sigma_1^2$ and $\sigma_2^2$, respectively. Show that the random
variable   

$\quad\quad\quad\quad U_n = \frac{(\overline{X} - \overline{Y}) - (\mu_1 - \mu_2)}{\sqrt{(\sigma_1^2  + \sigma_2^2 )/n}}$  

satisfies the conditions of Theorem 7.4 and thus that the distribution function of $U_n$ converges
to a standard normal distribution function as $n \rightarrow \infty$. [Hint: Consider $W_i = X_i - Y_i$, for
$i = 1, 2,...,n$.]  


$\textbf{Solution:}$  

since $X_1, X_2,...,X_n$ and $Y_1, Y_2,...,Y_n$ are independent random samples, it is the case that $W_1 = X_1 - Y_1, ..., W_n = X_n - Y_n$ are independent random variable. 

- $E(W_i) = E(X_i - Y_i) = E(X_i) - E(Y_i)= \mu_1-\mu_2$   

- $V(W_i) = V(X_i-Y_i) = V(X_i) + V(Y_i) = \sigma_1^2 + \sigma_2^2$ since $X_i$ and $Y_i$ are independent.   

- $\overline{W} = \frac{1}{n}\sum\limits_{i = 1}^nW_i = \frac{1}{n}\sum\limits_{i = 1}^nX_i - \frac{1}{n}\sum\limits_{i = 1}^nY_i = \overline{X} - \overline{Y}$  


So we see that the $\{W_i\}$ satisfy the conditions of Theorem and so the $U_n$ as defined aboe converge to the standard normal distribution.  

-- 

### 7.47   

An airline finds that 5 percent of the persons who make reservations on a certain flight do not show up for the flight. If the airline sells 160 tickets for a flight with only 155 seats, what is the probability that a seat will be available for every person holding a reservation and planning to fly?  

$\textbf{Solution:}$   

Let Y = The number of people that don't show up. This is a binomial random variable with $n = 160$, $p = 0.05$, and $\sigma^2 = (0.05)(0.95) / 160 = 0.000296875$ 

We want to find $P(Y/160 \geq 5)$  Thinking of each the 160 people as a random sample, $X_i$ = 1 if $i^{th}$ person does not show up. we have:

$\frac{Y}{160} = \frac{1}{160}\sum\limits_{i = 1}^{160}X_i$. It makes since for the $X_i$ to be independent, and 160 is large so the central limit theorem applies..

Will use 4.5 instead of 5 to get a better approximation 
$P(Y/160 \geq 4.5/160) = P(\frac{\frac{Y}{160} - 0.05}{\sqrt{0.000296875}} \geq \frac{0.028125 - 0.05}{\sqrt{0.000296875}} \approx P(Z \geq$ -1.269583)$ The R code below gives the value.

````{r}
 pnorm(-1.269583, lower.tail = FALSE)
````

### 7.51   

The manager of a supermarket wants to obtain information about the proportion of customers who dislike a new policy on cashing checks.  How many customers should he sample if he wants the sample fraction to be within 0.15 units of the true fraction, with probability 0.98?

$\textbf{Solution:}$  

Let Y = the number of people who dislike the new policy, and let n be the number of customers sampled, then we want to find  

$P(\big| Y/n - p \big| \leq 0.15)$  

Just as in the previous problem we can consider a random sample of n, $X_1,...,X_n$ people and notice that 
$Y/n = \sum\limits_{i = 1}^nX_i = \overline{}$  Now dividing by the standard deviation $= \sqrt{np(1-p)}$ allow us to use the central limit theorem  

$P(\big| Y/n - p \big| \leq 0.15) = 0.98 \approx P(|Z| \leq \frac{0.15}{\sqrt{p(1-p)/n}}) = 0.98$  
to find the quantile that corresponds to (1 - 0.98)/2, use R  

````{r}
qnorm(0.01, lower.tail = FALSE)
````

so we must have  $\frac{0.15}{\sqrt{p(1-p)/n}} = 2.053749$  We do not know p, but the previous problem says that the worst case variance occurs when $p = 0.5$, so it makes sense to use this value. 

$\sqrt{n}\frac{0.15}{.5} = 2.326348 \Rightarrow  n = (2.326348\cdot0.5/0.15)^2 = 60.13217$   

as usual we round up and this give n =61  


### Chapter 8   

### 8.2    

Suppose that $E(\hat{\Theta}_1) = E(\hat{\Theta}_2) = \Theta$, $V(\hat{\Theta}_1) = \sigma_1^2$, and  $V(\hat{\Theta}_2) = \sigma_2^2$.  Consider the estimator $\hat{\Theta}_3 = a\hat{\Theta}_1 + (1-a)\hat{\Theta}_2$  

a: Show that $\hat{\Theta}_3$ is an unbiased estimator for $\Theta$.   

  $\textbf{Solution:}$  
     
  So we need to show that $E(\hat{\Theta}_3) = \Theta$.  
    
  $E(\hat{\Theta}_3) = E(a\hat{\Theta}_1 + (1-a)\hat{\Theta}_2) = aE(\hat{\Theta}_1) + (1-a)E(\hat{\Theta}_2) = \Theta$ as was to be shown.  
    
b: If $\hat{\Theta}_1)$ and $\hat{\Theta}_2$ are independent, how should the constant a be chosen in order to minimize the variance of $\hat{\Theta}_3$?   
  
  $\textbf{Solution:}$ 
    
  $V(\hat{\Theta_3}) = V(a\hat{\Theta}_1 + (1-a)\hat{\Theta}_2) = a^2V(\hat{\Theta}_1) + (1 - a)^2 V(\hat{\Theta}_2) = a^2\sigma_1^2 + (1 - a)^2\sigma_2^2$  
    
  So we will minimize the function $f(a) = a^2\sigma_1^2 + (1 - a)^2\sigma_2^2$  
    
  $f'(a) = 2a\sigma_1^2 - 2(1 - a)\sigma_2^2 = 0 \Rightarrow a = \frac{\sigma_2^2}{\sigma_1^2 + \sigma_2^2}$     
  $f''(a) = 2\sigma_1^2 + 2\sigma_2^2 > 0$ so this is a minimum 

### 8.8        

the reading on a voltage meter connected to a test circuit is uniformly distributed over the interval $(\Theta, \Theta + 1)$, where $\Theta$ is the true, but unknown voltage of the circuit. Suppose that $Y_1,..., Y_n$ denotes a random sample of such readings.  

a: Show that $\overline{Y}$ is a biased estimator of of $\Theta$ and find the bias.  

  $\textbf{Solution:}$   
    
  We want to show that $E(\overline{Y}) = E(\frac{1}{n}\sum\limits_{i = 1}^n Y_i) \neq \Theta$.    
    
  notice that $E(Y_i) =  \Theta + \frac{1}{2}$  so 
    
  $E(\overline{Y}) = \frac{1}{n}\sum\limits_{i = 1}^n E(Y_i) = \Theta + \frac{1}{2} \neq \Theta$ 
    
  $B(\overline{Y}) = E(\overline{Y}) - \Theta = \frac{1}{2}$  
    
b: Find a function of $\overline{Y}$ which is an unbiased estimator of $\Theta$.  

  $\textbf{Solution:}$    
    
  Clearly $\overline{Y} - \frac{1}{2}$ is an unbiased estimator of $\Theta$.  
   
c: Find $MSE(\overline{Y})$ when $\overline{Y}$ is used as an estimator.  
    
  $\textbf{Solution:}$  
  
  $MSE(\overline{Y}) = V(\overline{Y}) + (B(\overline{Y}))^2)$  so we need to find  $V(\overline{Y})$
  
  Now, for wach $i$, $V(Y_i) = \frac{1}{12}$ so since the $Y_i$ are independent we have   
  $V(\overline{Y}) = \left(\frac{1}{n}\right)^2\sum\limits_{i = 1}^nV(Y_i) = \frac{1}{12n}$  
  
  So $MSE(\overline{Y}) = \frac{1}{12n} + \frac{1}{4}$  
  
   
### 8.11   

Let $Y_1,...,Y_n$ denote a random sample of size n from a population whose density is given by  

$f(y) = \begin{cases} 3\beta^3y^{-4} & \beta \leq y \\ 0 & \text{elsewhere} \end{cases}$   

where $\beta > 0$ is unknown. (This is one of the Pareto distributions introduced in Exercise 6.14) Consider the estimator $\hat{\beta} = min(Y_1,...,Y_n)$.  

a: Derive the bias of the estimator $\hat{\beta}$  
  
  $\textbf{Solution:}$   
  
  The first order statistic $Y_{(1)} = min(Y_1,...,Y_n)$ has density function given by $n[1 - F(y)]^{n-1}f(y)$  
  
  From Exercise 6.14, the distribution function of this Pareto distribution is given by  
  
  $F(y) = \begin{cases} 1 - \left(\frac{\beta}{y}\right)^3 & \beta \leq y \\ 0 & \text{elsewhere} \end{cases}$  
  
  So the density function of $Y_{(1)}$ is given by  
  
  $f_{Y_{(1)}} = \begin{cases} n\left[\left(\frac{\beta}{y}\right)^3 \right]^{n-1}3\beta^3y^{-4} & \beta \leq y \\ 0 & \text{elsewhere} \end{cases} = \begin{cases} 3n\beta^{3n}y^{-3n-1} & \beta \leq y \\ 0 & \text{elsewhere} \end{cases}$  so  
  
  $E(Y_{(1)}) = \int\limits_{\beta}^{\infty}3n\beta^{3n}x^{-3n}dx = \frac{3n}{-3n + 1}\beta^{3n}y^{-3n+1}\bigg|_{\beta}^{\infty} = \frac{3n}{3n - 1}\beta$  
  
  So $B(\overline{Y}) = E(Y_{(1)}) - \beta = (\frac{3n}{3n - 1} - 1)\beta = \frac{\beta}{3n - 1}$
  
  $E(Y_{(1)}^2) = \int\limits_{\beta}^{\infty}3n\beta^{3n}x^{-3n+1}dx =\frac{3n}{-3n + 2}\beta^{3n}y^{-3n+2}\bigg|_{\beta}^{\infty} = \frac{3n}{3n - 2}\beta^2$ 
  
b: Derive $MSE(\hat{\beta})$.  

  $\textbf{Solution:}$  
  
  $MSE(\hat{\beta}) = MSE(Y_{(1)}) = E((Y_{(1)}) - \beta)^2) = E(Y_{(1)}^2 - 2\beta Y_{(1)} + \beta^2) = E(Y_{(1)}^2 - 2\beta E(Y_{(1)}) + \beta^2$   
  
  so need to calculate $E(Y_{(1)}^2)$  
  
  $E(Y_{(1)}^2) = \int\limits_{\beta}^{\infty}3n\beta^{3n}x^{-3n+1}dx =\frac{3n}{-3n + 2}\beta^{3n}y^{-3n+2}\bigg|_{\beta}^{\infty} = \frac{3n}{3n - 2}\beta^2$  so  
  
  
 $MSE(\hat{\beta} = \frac{3n\beta^2}{3n - 2} - \frac{6n\beta^2}{3n - 1}  + \beta^2 = \frac{3n(3n-1)\beta^2 -6n(3n-2)\beta^2 + (3n-1)(3n-2)\beta^2}{(3n - 2)(3n -1)} = \frac{2\beta^2}{(3n - 2)(3n -1)}$
  

### 8.12    

Suppose that $Y_1,...,Y_n$ constitute a random sample from a normal distribution with parameters $\mu$ and $\sigma^2$  

a: Show that $S = \sqrt{S^2}$ is a biased estimator of $\sigma$. [Hint: Recall the distribution of $\frac{(n-1)S^2}{\sigma^2}$ and the result of Exercise 4.90].  

  $\textbf{Solution:}$  
  
  $\frac{(n-1)S^2}{\sigma^2}$ has a $\chi^2$ distribution with $n-1$ degrees of freedom. 
  
  From Exercise 4.90  
  $(*) \quad E(Y^a) = \frac{2^{a}\Gamma(\frac{n-1}{2} + a)}{\Gamma(\frac{n-1}{2})}$ 
  
  $E(\hat{\sigma}) = E(\sqrt{S^2}) = \frac{\sigma}{\sqrt{n-1}}E(\left( \sqrt{\frac{n-1}{\sigma^2}S^2}\right)^{0.5})  = \frac{\sigma}{\sqrt{n-1}}\frac{\sqrt{2}\Gamma(\frac{n-1}{2} + \frac{1}{2})}{\Gamma(\frac{n-1}{2})} = \sigma\frac{\sqrt{2}\Gamma(\frac{n}{2})}{\sqrt{n-1}\Gamma(\frac{n-1}{2})}$ ($(*)$ was used here)  
    
b: Adjust S to form an unbiased estimator of $\sigma$.  
  
  $\textbf{Solution:}$  
  
  Just multiply by the one over $\frac{\sqrt{2}\Gamma(\frac{n}{2})}{\sqrt{n-1}\Gamma(\frac{n-1}{2})}$  
  
  so  
  
  $\hat{\sigma} = \frac{\sqrt{n-1}\Gamma(\frac{n-1}{2})}{\sqrt{2}\Gamma(\frac{n}{2})} S$  
   
   
C: Find an unbiased estimator of $\mu - z_{\alpha}\sigma$, the point that cuts off a lower-tail area of $\alpha$ under this normal curve.  

  $\textbf{Solution:}$  
  
  Note: $E(\overline{Y}) = \mu$    
  
  This together with the estimator from part b) give an estimator for $\mu - z_{\alpha}\sigma$  
  
  $\overline{Y} - z_{\alpha} \frac{\sqrt{n-1}\Gamma(\frac{n-1}{2})}{\sqrt{2}\Gamma(\frac{n}{2})}S$  
  
  Now $E(\overline{Y} - z_{\alpha} \frac{\sqrt{n-1}\Gamma(\frac{n-1}{2})}{\sqrt{2}\Gamma(\frac{n}{2})}S) = E(\overline{Y}) - z_{\alpha} E(\frac{\sqrt{n-1}\Gamma(\frac{n-1}{2})}{\sqrt{2}\Gamma(\frac{n}{2})}S) = \mu - z_{\alpha}\sigma$  So it is unbiased.  
  
  
  
  
  

